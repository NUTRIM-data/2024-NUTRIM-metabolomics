---
title: "Day 2 - Practical 2 - more advanced methods for metabolomics data analysis"
subtitle: "2024 NUTRIM microbiome & metabolome workshop"
author: Michal Skawinski
editor: visual
date: last-modified
keep-md: false
theme: 
  light: flatly
  dark: darkly
format: html
embed-resources: true
code-block-border-left: true
code-block-bg: true
toc: true
toc-location: right
toc-depth: 4
toc-expand: 3
number-sections: true
number-depth: 3
fig-align: center
fig-dpi: 300
fig-width: 7.5
fig-height: 5
fig-responsive: true
code-tools: true
code-fold: false
code-link: true
code-annotations: below
lightbox: auto
cache: false
bibliography: references.bib
---

# Intro üîÜ

In this tutorial, we are going to explore some more advanced methods and how to implement them in R:

-   [**URF**](#urf) (Unsupervised Random Forest), and

-   [**ASCA**](#asca) (ANalysis Of VAriance (ANOVA)-Simultaneous Component Analysis).

# Load R packages and some functions üì¶

```{r}
#| output: false
library(here)
library(tidyverse) # data cleaning 
library(ggplot2) # visualizations
source(here("functions.R"))  # URF functions
library(randomForest) 
library(factoextra) 
library(limpca) # ASCA

```

# Read the data üîç

We are going to use the same data as before, so you should have it already stored in your environment. However if you restarted the R session, you can use the code below to read the data again:

```{r}
# Read metabolites data table
metabolites <- read.csv(file = here("data:papa2012/processed/metabolites_short.csv"), row.names = 1)

# PQN normalize it
metabolites_pqn <- pqn(metabolites)

# Read metadata
metadata_metabolites <- read.csv(file = here("data:papa2012/processed/metadata_metabolites.csv"), row.names = 1)
```

# URF üå≤üå≤üå≤ {#urf}

We have provided a function to perform URF in R:

```{r}
#| output: asis
#| eval: false
script_URF(nr_itteration, real_data, nr_trees, nr_samples, class) 
```

##### **TASK 1**

Look at the code of the URF function below. You don't need to understand or analyze the body of the function. Just look at the comments and try to analyze the inputs and outputs.

What are the input arguments ? What are the outcome(s) of the function? Tip: Look what's inside the `return`.

```{r}
#| eval: false
#| output: asis
script_URF <- function(nr_itteration, real_data, nr_trees, nr_samples) {
  #Input arguments:
  # nr_itteration --> number of times the procedure should be repeated; e.g.50, 100
  # real_data --> data set (samples in row and variables in column
  # nr_trees --> number of trees in the RF; 1500 or more
  # nr_samples --> number of samples in a  terminal nodes, default is 1 but better to put higher number 5 or 8
  
  #Outputs:
  #pc --> pca score plot
  #pr--> % of variance per PC
  #mean proximity matrix
  
  require(randomForest)
  require(factoextra)
  
  proximity_all <- array(NA, dim = c(nrow(real_data), nrow(real_data), nr_itteration))
  for(i in 1:nr_itteration) {
    cat(i, "\n")
    result <- unsupervisedRF_proximityadd(real_data, nr_trees, nr_samples) #[b_tree_3_classes2,proximity_realdata]=unsupervisedRF_proximityadd(real_data,nr_trees,nr_samples);
    proximity_all[, ,i] <- result$proximity_realdata  #proximity_all(:,:,i)=proximity_realdata;
  }
  
  # PCA on the mean value of proximity
  mean_proximity <- apply(proximity_all, c(1,2), mean)
  
  pc <- prcomp(doublecentering(1-mean_proximity),
               center = FALSE,
               scale = FALSE)
  
  pc.eigval <- get_eigenvalue(pc)
  
  return(list(pc = pc,
              pr = pc.eigval,
              mean_proximity = mean_proximity))
}
```

##### **TASK 2**

Let's perform our first URF! Call a function with 1000 trees and 8 samples in a terminal node. Limit the number of iterations up to 10, which will speed up the computations (generally we would use around 100 iterations). Assign the results to an object named `URF`.

::: {.callout-note collapse="true"}
### Do you need to scale the data?

URF is an non-linear method, therefore it will perform well with non-scaled data.
:::

```{r}
# write your code in the notebook
```

::: {.callout-note collapse="true" appearance="simple"}
##### SOLUTION:

```{r}
#| message: false
#| output: false
URF <- script_URF(nr_itteration = 1, real_data = metabolites_pqn, nr_trees = 1000, nr_samples = 8)
```
:::

##### **TASK 3**

Look at the structure of the resulting URF model:

```{r}
str(URF)
```

##### **TASK 4**

Plot the URF score plot. Color the points by `Diagnosis`. What do you observe?

```{r}
ggplot(as.data.frame(URF$pc$x), 
       aes(x = PC1, 
           y = PC2,
           color = metadata_metabolites$Diagnosis)) + 
  geom_point() + 
  labs(x = paste0("PC1 (",round(URF$pr[1,2],2),"%)"), 
       y = paste0("PC2 (",round(URF$pr[2,2],2),"%)")) + 
  theme_bw()
```

# ASCA {#asca}

There are many versions of ASCA, depending on the application and **data design**. Today, we will only familiarize with one version: ASCA+ from the R package `limpca` [@thiel2023].

More information about the package, as well as tutorials and examples with datasets can be found [here](https://bioconductor.org/packages/release/bioc/html/limpca.html).

## ASCA - case study

We are going to inspect the effect of the disease (nonIBD, UC, CD) and the patient age group (minors vs adults) on the metabolites' profiles. We suspect, that the effect of the disease on metabolites would differ based on the age (between minors and adults).

In our data, we have multiple samples per participant. This comes from sampling of patients at different times. However, we are not interested in this factor as it doesn't follow any design.

Therefore, we have provided a new file `metadata_metabolites_asca.csv` with the metadata of only selected samples from one time point (one sample per patient).

##### **TASK 1**

Read `metadata_metabolites_asca.csv`. How many samples do we have now?

```{r}
# write your code in the notebook
```

::: {.callout-note collapse="true" appearance="simple"}
```{r}
metadata_metabolites_asca <- read.csv(file = here("data:papa2012/asca/metadata_metabolites_asca.csv"), row.names = 1)

nrow(metadata_metabolites_asca)
```
:::

##### **TASK 2**

We also need to select those samples from the metabolites data table (PQN normalized).

```{r}
metabolites_asca <- metabolites_pqn[which(rownames(metabolites_pqn) %in% metadata_metabolites_asca$SampleID), ]
```

##### **TASK 3**

In the metadata, we have only variable `Age`, however no age groups. But, we can stratify our patients based on their age. Let's create a vector `Age_group` by stratifying `Age` variable to minors (below 18) and adults, and add it to `metadata_metabolites_asca`. Tip: use `ifelse()` function.

```{r}
# write your code in the notebook
```

::: {.callout-note collapse="true" appearance="simple"}
##### SOLUTION:

```{r}
Age_group <- ifelse( metadata_metabolites_asca$Age <18, "minor", "adult")

metadata_metabolites_asca$Age_group <- Age_group
```
:::

##### **TASK 4**

Let's see, if our data is balanced now. Tip: use `table()` and provide both `Age_group` and `Diagnosis` from our metadata as arguments.

```{r}
# write your code in the notebook
```

##### SOLUTION:

```{r}
table(Age_group, metadata_metabolites_asca$Diagnosis)
```

:::

## Create an ASCA object

In order to use ASCA from the `limpca` package, the data need to be formatted as a list with the following elements:

-   outcomes (multivariate matrix with the responses - our metabolites data),

-   design (data.frame - our metadata),

-   formula (character string with factors and interactions).

First, create an empty list:

```{r}
ASCA_model <- list()
```

##### **Outcomes**

Outcomes should contain our metabolites data Append `outcomes` to the list. We need to transform our data, since ASCA uses linear models, which assume normal distribution:

```{r}
ASCA_model$outcomes <- as.matrix(log(metabolites_asca+1))
```

##### **Design**

Design has a form of a data frame with the values of our factors. Make sure to transform the variables into factors:

```{r}

ASCA_model$design <- metadata_metabolites_asca %>% 
  dplyr::select(c("Diagnosis", "Age_group")) %>%
  dplyr::mutate(Diagnosis = factor(Diagnosis, levels = c("nonIBD", "CD", "UC")),
                Age_group = factor(Age_group, levels = c("minor", "adult")))

# Row names of the design matrix need to be the same as of the outcomes (sample names)
rownames(ASCA_model$design) <- metadata_metabolites_asca$SampleID
```

We can use the function `plotDesign()` to plot the design matrix:

```{r}
plotDesign(design = ASCA_model$design, x = "Diagnosis", y = "Age_group",
    title = "Design of the metabolomics dataset")
```

Look's familiar? We have already inspected the design matrix by using `table` above. And our design has a good balance.

##### **Formula**

In the formula, we need to specify all factors of interest and the interactions between them:

::: {.callout-note collapse="true"}
### What is an interaction ?

According to ChatGPT, "An interaction term in a linear model represents the combined effect of two or more predictor variables on the response variable, beyond their individual effects. It allows the model to account for situations where the effect of one predictor on the response depends on the level of another predictor."

What does it really mean? Let's think of a simple example: we want to predict the results of an *exam* based on the *study hours* and *use of the study guide*. Without an interaction, the result will be a sum of the contributions of study hours and the use of the study guide. But the time we spend studying depends on whether we use the study guide or not, as we might study less but get a higher score on the exam in the end.

To sum up, an interaction allows the effect of one predictor to change depending on the level of another predictor, depiicting a more complex relationship between the predictors and the outcome.
:::

```{r}
ASCA_model$formula <- "outcomes ~ Diagnosis + Age_group + Diagnosis:Age_group"
```

## Perform ASCA

### Model matrix generation

The first step of ASCA+ is to encode our design matrix as a model matrix to build linear models. Each factor and interaction is encoded with multiple binary variables:

```{r}
resLmpModelMatrix <- lmpModelMatrix(ASCA_model)
```

Print the model matrix:

```{r}
head(resLmpModelMatrix$modelMatrix)
```

### Decomposition of effect matrices

The next step is to decompose the model matrix with linear models for every factor or interaction separately:

```{r}
resLmpEffectMatrices <- lmpEffectMatrices(resLmpModelMatrix)
```

This results in creation of so called effect matrices for every factor and interaction.

### ASCA

Now, we can apply PCA simultaneously to all effect matrices (therefore SCA- simultaneous component analysis).

::: {.callout-note collapse="true"}
### Combined effect

We can also combine the factors and interactions (by linear combinations) and apply PCA to the combined effect matrix, e.g. `Diagnosis + Age_group + Diagnosis:Age_group`, however then the interpretation gets more difficult.
:::

Perform PCA of each of the factors/interactions:

```{r}
resASCA <- lmpPcaEffects(resLmpEffectMatrices = resLmpEffectMatrices,
                         method = "ASCA")
```

### Contributions

##### **Contribution to the total variance**

From ASCA we obtain scores and loadings for each factor and interaction. We can display the **total contribution** of each factor and interaction to the total variance in our data (in %):

```{r}
resLmpContributions <- lmpContributions(resASCA)

resLmpContributions$totalContribTable
```

The values should add up to 100%.

##### **PCs contributions in each model**

We can also display the % of variance explained by each PC in each PCA model for the factors and interactions:

```{r}
resLmpContributions$effectTable
```

Here, PCs from each factor/interaction sums up a 100% for that PCA model. Note that we have more PCs for the residuals, but we only display the first 5.

### **Statistical testing**

[WARNING: don't execute this part in your notebook, it will crush the Posit environment! Instead, look at the results below.]{style="color: red"}

Okay, we've got the total contribution of each factor/interaction. Based on that, we see that `Diagnosis` contributes to 24.3% of variance in metabolites' profiles. But how can we know if this effect is significant? Here, we use bootstrapping with a big number of iterations, which will give us the p-values.

To test the significant of model effects by bootstrap with a reduced number of iterations (e.g. 100) iterations, which will reduce the computation time:

```{r}
resLmpBootstrapTests <- lmpBootstrapTests(resLmpEffectMatrices = resLmpEffectMatrices, 
                                          nboot = 1000)
```

Display P-values :

```{r}
t(resLmpBootstrapTests$resultsTable)
```

This way, we display the total contribution of each factor/interaction and additionaly their p-values.

### Scores and loadings plots

Each PCA model for a factor/interaction or a PCA model for the combined effect results in **scores** and **loadings**. That's why ASCA is a great tool, because it provides interpretability of the results.

`Diagnosis` was significant from the bootsraping, but let's visualize a **score plot** for this factor:

```{r}
#| code-fold: true
lmpScorePlot(resASCA, effectNames = "Diagnosis",
                          color = "Diagnosis")
```

And a loadings plot for the first PC:

```{r}
#| code-fold: true
lmpLoading1dPlot(resASCA, effectNames = "Diagnosis",
                 axes = 1, # only PC1
                  xaxis_type = "character",)
```

For the `Diagnosis` effect, the score plots show a clear separation between the different levels on the first PC. The distinct separation of the groups suggests a strong effect of that factors, which is also confirmed by it's significance.

::: {.callout-note collapse="true"}
### What about the interaction?

We can also visualize the scores and loadings plots for the interaction `Diagnosis:Age_group`:

```{r}
#| code-fold: true
lmpScorePlot(resASCA, effectNames = "Diagnosis:Age_group",
                          color = "Diagnosis", shape = "Age_group")
```

Based on the scores plot, the interaction seems important, but its not significant in bootstrapping.

Interactions can be further visualized with line plots:

```{r}
#| code-fold: true
lmpEffectPlot(resASCA, effectName = "Diagnosis:Age_group", x = "Diagnosis",
              z = "Age_group")

lmpEffectPlot(resASCA, effectName = "Diagnosis:Age_group", x = "Age_group",
              z = "Diagnosis")
```

Based on the interaction plots, UC and CD groups are more similar for all ages, but the minors and adults nonIBD patient differ.
:::

# That's all folks!

Now you understand URF and ASCA and you should be able to apply them in R to your data!

# Session info

<details>

```{r}
sessioninfo::session_info()
```

</details>
